{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7214ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and print the stock tickers that make up S&P500\n",
    "tickers = pd.read_html(\n",
    "    'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5cf8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting and ending point of the historical serie\n",
    "start_date = \"2006-01-01\"\n",
    "end_date = \"2010-01-01\"\n",
    "\n",
    "# create an array of interested tickers\n",
    "ticker_symbols = np.array(tickers[tickers['Date added'] < start_date]['Symbol'])\n",
    "print(f'number of tickers up to now = {len(ticker_symbols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#downloading data of the first ticker from yahoo finance\n",
    "df = yf.download(ticker_symbols[0], start=start_date, end=end_date).drop(\n",
    "    columns=[ 'Open', 'High', 'Low']).rename(\n",
    "    \n",
    "    columns={\n",
    "        \n",
    "        'Close': 'close_'+ticker_symbols[0],\n",
    "        'Adj Close': 'adj_close_'+ticker_symbols[0],\n",
    "        \"Volume\": \"volume_\"+ticker_symbols[0],\n",
    "        \n",
    "    })\n",
    "\n",
    "for i in range(1, len(ticker_symbols)):\n",
    "\n",
    "    #downloading data from yahoo finance\n",
    "    dh = yf.download(ticker_symbols[i], start=start_date, end=end_date).drop(\n",
    "        columns=[ 'Open', 'High', 'Low']).rename(\n",
    "\n",
    "        columns={\n",
    "\n",
    "            'Close': 'close_'+ticker_symbols[i],\n",
    "            'Adj Close': 'adj_close_'+ticker_symbols[i],\n",
    "            \"Volume\": \"volume_\"+ticker_symbols[i],\n",
    "\n",
    "        })\n",
    "    \n",
    "    df = pd.concat([df, dh], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check about the presence of NaN values in the dataset\n",
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check those tickers that have some NaN values and delete them from the dataset\n",
    "nan_tickers= {}\n",
    "trading_days = df.shape[0]\n",
    "for l in ticker_symbols:\n",
    "    \n",
    "    if (df['close_'+l].isnull().sum() + df['volume_'+l].isnull().sum() + df['adj_close_'+l].isnull().sum()) > 0:\n",
    "        \n",
    "        # percentage of NaN values with respect the total trading days in close, adj_close and volume\n",
    "        nan_tickers[l] = [\n",
    "            \n",
    "            round(100*df['close_'+l].isnull().sum()/trading_days),\n",
    "            round(100*df['adj_close_'+l].isnull().sum()/trading_days),\n",
    "            round(100*df['volume_'+l].isnull().sum()/trading_days)\n",
    "                         ]\n",
    "delete_tickers = list(nan_tickers.keys())\n",
    "print(f'number of tickers with NaN values = {len(delete_tickers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b266b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete those tickers that have some NaN values\n",
    "# first build the list of columns to delete\n",
    "drop_columns = []\n",
    "for l in delete_tickers:\n",
    "    drop_columns.append('close_' + l)\n",
    "    drop_columns.append('adj_close_' + l)\n",
    "    drop_columns.append('volume_' + l)\n",
    "\n",
    "df = df.drop(\n",
    "    columns= drop_columns\n",
    ")\n",
    "\n",
    "\n",
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'final number of tickers = {round(df.shape[1]/3)}')\n",
    "print(f'number of tickers with NaN values = {len(delete_tickers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def. of new and final list of tickers\n",
    "final_tickers = list(ticker_symbols)\n",
    "for l in delete_tickers:\n",
    "    final_tickers.remove(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT the DATASET as txt file\n",
    "# OBS. : actually this format is not so useful !\n",
    "\n",
    "#df.to_csv('data/complete_dataset.txt', index=False, float_format='%.15g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829d14d",
   "metadata": {},
   "source": [
    "### Export all the useful datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36bc58",
   "metadata": {},
   "source": [
    "**Obs.**: ChatGPT suggests me to export the datset as txt file instead of csv file. <br>\n",
    "It says that txt usually preserves better the precision of the numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT the final list of TICKERS\n",
    "pd.DataFrame(final_tickers, columns=['ticker']).to_csv('data/ticker.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the index to a DatetimeIndex\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# convert the date index to a list of strings\n",
    "date_strings = df.index.date.astype(str).tolist()\n",
    "\n",
    "# EXPORT the DATES as txt file\n",
    "pd.DataFrame(date_strings, columns=['date']).to_csv('data/date.txt', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ebffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the two dataframe (the one exported and the one imported) are the same\n",
    "\n",
    "print(pd.read_csv('data/date.txt').equals(pd.DataFrame(date_strings, columns=['date'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c88e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT CLOSE PRICES\n",
    "df_close = df.iloc[:, ::3] \n",
    "df_close.columns = final_tickers\n",
    "df_close.to_csv('data/close.txt', index=False, float_format='%.15g')\n",
    "\n",
    "# EXPORT ADJUSTED CLOSE PRICE\n",
    "df_adj_close = df.iloc[:, 1::3]\n",
    "df_adj_close.columns = final_tickers\n",
    "df_adj_close.to_csv('data/adj_close.txt', index=False, float_format='%.15g')\n",
    "\n",
    "# EXPORT VOLUME\n",
    "df_volume = df.iloc[:, 2::3]\n",
    "df_volume.columns = final_tickers\n",
    "df_volume.to_csv('data/volume.txt', index=False, float_format='%.15g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT CLOSE PRICES' RETURNS\n",
    "df_close_return = (df_close / df_close.shift(1) - 1).dropna()\n",
    "df_close_return.to_csv('data/close_return.txt', index=False, float_format='%.15g')\n",
    "\n",
    "# EXPORT ADJUSTED CLOSE PRICES' RETURNS\n",
    "df_adj_close_return = (df_adj_close / df_adj_close.shift(1) - 1).dropna()\n",
    "df_adj_close_return.to_csv('data/adj_close_return.txt', index=False, float_format='%.15g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e181a98",
   "metadata": {},
   "source": [
    "**comparison** between the imported dataset<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b537301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these values of atol and rtol are the standard ones\n",
    "pd.testing.assert_frame_equal(\n",
    "    # change the index to the standar one, otherwise it'll raise an error\n",
    "    df_adj_close_return.reset_index().drop(columns='Date'), \n",
    "    pd.read_csv('data/adj_close_return.txt'), \n",
    "    rtol=1e-5,\n",
    "    atol=1e-8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae1b90",
   "metadata": {},
   "source": [
    "*Note by ChatGPT:* <br>\n",
    "if `pd.testing.assert_frame_equal()` doesnâ€™t raise an exception, it means the two DataFrames are equal. If there are differences, it will raise an error detailing the mismatch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
